{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f50b39",
   "metadata": {
    "id": "53f50b39"
   },
   "source": [
    "## URL to Reference\n",
    "https://pastebin.com/J3HARz0g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a97298",
   "metadata": {},
   "source": [
    "# HK01 News Scraper\n",
    "This notebook outlines the process of scraping news articles from HK01 based on different topic IDs. We extract key details such as article ID, URL, description, publish time, and full content. This script is designed to be a robust and efficient way to gather news data for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a9a3745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.5.2 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from -r ../requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: Requests==2.31.0 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from -r ../requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm==4.66.1 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from -r ../requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from pandas==1.5.2->-r ../requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from pandas==1.5.2->-r ../requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from pandas==1.5.2->-r ../requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from Requests==2.31.0->-r ../requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from Requests==2.31.0->-r ../requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from Requests==2.31.0->-r ../requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from Requests==2.31.0->-r ../requirements.txt (line 2)) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.5 in /Users/felixlui/opt/anaconda3/envs/PersonalProject/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.5.2->-r ../requirements.txt (line 1)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3352dc09",
   "metadata": {
    "id": "3352dc09"
   },
   "outputs": [],
   "source": [
    "import requests  # To make HTTP requests to HK01\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import json  # To parse JSON responses\n",
    "import itertools  # For efficient looping\n",
    "from tqdm import tqdm  # For displaying progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "xlZX-Nfny9aJ",
   "metadata": {
    "id": "xlZX-Nfny9aJ"
   },
   "outputs": [],
   "source": [
    "def fetch_news_data(tag_ids: list):\n",
    "    \"\"\"\n",
    "    Fetches news data from HK01 for given tag IDs and creates a DataFrame.\n",
    "\n",
    "    Each tag ID corresponds to a specific news category. This function makes HTTP requests\n",
    "    to the HK01 API, retrieves news articles for each tag ID, and compiles the data into\n",
    "    a structured DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - tag_ids (list of int): A list of tag IDs for which to fetch news articles.\n",
    "      Example tag IDs: 港鐵系統失靈 is 347, 港鐵班次延誤 is 348.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the following columns:\n",
    "      ['articleId', 'publishUrl', 'description', 'publishTime'].\n",
    "      Each row corresponds to a unique news article.\n",
    "    \n",
    "    Notes:\n",
    "    - The function iterates through each tag ID and makes paginated requests to the API.\n",
    "    - Data for each article includes its ID, URL, description, and publish time.\n",
    "    - Duplicate articles (based on 'articleId') are dropped to ensure uniqueness.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for element in tag_ids:\n",
    "        offset = \"<OFFSET>\"\n",
    "        while True:\n",
    "            hk01_api_article_list = f\"https://web-data.api.hk01.com/v2/feed/tag/{element}?offset={offset}\"\n",
    "            response = requests.get(hk01_api_article_list)\n",
    "            \n",
    "            # Check for successful response\n",
    "            if response.status_code == 200:\n",
    "                json_data = response.json()\n",
    "                try:\n",
    "                    # Update offset for pagination\n",
    "                    new_offset = str(json_data[\"nextOffset\"])\n",
    "                    if new_offset:\n",
    "                        offset = new_offset\n",
    "                        new_data = [[i[\"data\"].get(\"articleId\"), \n",
    "                                     i[\"data\"].get(\"publishUrl\"), \n",
    "                                     i[\"data\"].get(\"description\"), \n",
    "                                     i[\"data\"].get(\"publishTime\")] \n",
    "                                    for i in json_data.get(\"items\", [])]\n",
    "                        data.append(new_data)\n",
    "                except KeyError:\n",
    "                    # Break the loop if no further data is available\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for tag {element}. HTTP Status Code: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "    # Flatten the list of data and create DataFrame\n",
    "    flatten_data = list(itertools.chain(*data))\n",
    "    df = pd.DataFrame(flatten_data, columns=[\"articleId\", 'publishUrl', 'description', \"publishTime\"])\n",
    "    \n",
    "    # Remove duplicate entries based on articleId\n",
    "    return df.drop_duplicates(subset=[\"articleId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "O8F9AUx00p40",
   "metadata": {
    "id": "O8F9AUx00p40"
   },
   "outputs": [],
   "source": [
    "def add_article_summary_and_format_time(df):\n",
    "    \"\"\"\n",
    "    Enhances the DataFrame by adding a summary for each article and formatting the publish time.\n",
    "\n",
    "    This function iterates through each article ID in the DataFrame, fetches its summary (teaser)\n",
    "    from the HK01 API, and appends it to the DataFrame. It also formats the publish time of each\n",
    "    article to a readable datetime format.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): A DataFrame containing at least 'articleId' and 'publishTime' columns.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The enhanced DataFrame with an added 'Summary' column and formatted 'publishTime'.\n",
    "\n",
    "    Notes:\n",
    "    - The function retrieves the teaser of each article using its 'articleId'.\n",
    "    - The 'publishTime' column is converted from Unix timestamp to a datetime object for readability.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare a list to store the summaries\n",
    "    summaries = []\n",
    "\n",
    "    # Iterate through each article ID to fetch its summary\n",
    "    for article_id in df['articleId']:\n",
    "        try:\n",
    "            hk01_article_detail = f\"https://web-data.api.hk01.com/v2/page/article/{article_id}\"\n",
    "            response = requests.get(hk01_article_detail)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                detail_dict = response.json()\n",
    "                teaser = detail_dict[\"article\"].get(\"teaser\", \"\")\n",
    "                summaries.append(teaser)\n",
    "            else:\n",
    "                print(f\"Failed to fetch details for article {article_id}. HTTP Status Code: {response.status_code}\")\n",
    "                summaries.append(\"\")  # Append an empty string in case of failure\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error occurred while fetching article {article_id}: {e}\")\n",
    "            summaries.append(\"\")  # Append an empty string in case of error\n",
    "\n",
    "    # Adding the summaries to the DataFrame\n",
    "    df[\"Summary\"] = summaries\n",
    "\n",
    "    # Formatting the 'publishTime' column\n",
    "    df[\"publishTime\"] = pd.to_datetime(df[\"publishTime\"], unit=\"s\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qLNFomdW0rac",
   "metadata": {
    "id": "qLNFomdW0rac"
   },
   "outputs": [],
   "source": [
    "def extract_and_format_text(df):\n",
    "    \"\"\"\n",
    "    Extracts and formats the content of each article in the DataFrame.\n",
    "\n",
    "    Iterates over each article ID in the DataFrame, makes a request to the HK01 API to fetch\n",
    "    the article's content, and formats it. The formatted content is then appended to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): A DataFrame containing the 'articleId' column.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The enhanced DataFrame with an added 'allContent' column containing\n",
    "      the formatted text of each article.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses 'tqdm' to display a progress bar for the extraction process.\n",
    "    - The content is formatted by appending different types of text blocks, including bold text.\n",
    "    - In case of an HTTP error or other request issues, an error message is appended.\n",
    "    \"\"\"\n",
    "\n",
    "    # List to store the formatted text of each article\n",
    "    article_list = []\n",
    "    \n",
    "    # Progress bar for extraction process\n",
    "    for article_id in tqdm(df['articleId'], desc=\"Extracting Article content\"):\n",
    "        formatted_text = \"\"\n",
    "\n",
    "        try:\n",
    "            url = f\"https://web-data.api.hk01.com/v2/page/article/{article_id}\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raises an error for HTTP error codes\n",
    "\n",
    "            article_data = response.json()\n",
    "\n",
    "            # Parsing and formatting the article content\n",
    "            for block in article_data[\"article\"][\"blocks\"]:\n",
    "                if block[\"blockType\"] == \"text\":\n",
    "                    for token_list in block.get(\"htmlTokens\", []):\n",
    "                        for token in token_list:\n",
    "                            if token[\"type\"] == \"boldText\":\n",
    "                                formatted_text += token[\"content\"] + \":\"\n",
    "                            elif token[\"type\"] == \"text\":\n",
    "                                formatted_text += token.get(\"content\", \"\")\n",
    "                        formatted_text += \"\\n\"\n",
    "            \n",
    "            article_list.append(formatted_text)\n",
    "        except requests.RequestException as e:\n",
    "            formatted_text = f\"Error fetching data: {e}\"\n",
    "            article_list.append(formatted_text)\n",
    "\n",
    "    # Appending the formatted content to the DataFrame\n",
    "    df[\"allContent\"] = article_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0444a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(df, filename):\n",
    "    \"\"\"\n",
    "    Exports the given DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to be exported.\n",
    "    - filename (str): The name of the output file. The function will add '.csv' extension\n",
    "      if not already present.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function writes the DataFrame to a CSV file and returns nothing.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses 'utf-8-sig' encoding, which is suitable for CSV files containing\n",
    "      special characters and ensures compatibility with most CSV readers, including Excel.\n",
    "    - The index of the DataFrame is not included in the output file.\n",
    "    - In case of any IOError, an appropriate message is printed.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Ensure the filename ends with '.csv'\n",
    "        if not filename.endswith('.csv'):\n",
    "            filename += '.csv'\n",
    "\n",
    "        # Exporting DataFrame to CSV\n",
    "        df.to_csv(filename, encoding=\"utf-8-sig\", index=False)\n",
    "        print(f\"File '{filename}' has been successfully saved.\")\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"Error occurred while saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c49627ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_this_pipeline(tag_id:list,filename:str):\n",
    "    df = fetch_news_data(tag_id)\n",
    "    df = add_article_summary_and_format_time(df)\n",
    "    df = extract_and_format_text(df)\n",
    "    output(df,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d6999ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while fetching article 207895: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Article content: 100%|██████████| 578/578 [08:44<00:00,  1.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'hk01_fulloutput.csv' has been successfully saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_this_pipeline([347,348,15217],\"hk01_fulloutput.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1052af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
